\documentclass[12pt, a4paper]{article}

% Настройка размера страницы
\usepackage[a4paper,bindingoffset=0.2in,%
            left=1in,right=1in,top=1in,bottom=1in,%
            footskip=.25in]{geometry}

% Настройка шрифтов
\usepackage[russian]{babel}
\usepackage{fontspec}
\setmainfont[
  Ligatures=TeX,
  Extension=.otf,
  BoldFont=cmunbx,
  ItalicFont=cmunti,
  BoldItalicFont=cmunbi,
]{cmunrm}
\usepackage{polyglossia}
\setdefaultlanguage{russian}
\setotherlanguage{english}
\PolyglossiaSetup{russian}{indentfirst=true}

% Для математики
\usepackage{amssymb,amsmath}

% Для множества колонок
\usepackage{multicol}

% Для отображения алгоритмов
\usepackage[ruled,linesnumbered,lined,commentsnumbered]{algorithm2e}
\SetKw{KwAnd}{and}

% Собственные обозначения для скобок
\newcommand{\roubr}[1]{\left(#1\right)}
\newcommand{\sqbr}[1]{\left[#1\right]}
\newcommand{\cubr}[1]{\left\{#1\right\}}

% Для отображения ссылок
\usepackage{hyperref}

\begin{document}
\title{Метод Ньютона для решения систем нелинейных уравнений}
\author{Шепрут Илья}
\maketitle

Всё будет описано для размерности $n = 2$, но это легко обобщается на более высокие размерности.

\setlength{\columnsep}{30pt}
\begin{multicols}{2}

\section{Формулировка}

Дана Система Нелинейных Уравнений (СНУ) в виде:

\begin{equation}\label{dano}
\left\{
\begin{aligned}
	&F_1(x_1,\ x_2) = 0 \\
	&F_2(x_1,\ x_2) = 0
\end{aligned}
\right.
\end{equation}

Требуется, чтобы она имела непрерывные производные $1$ порядка.

Необходимо найти такие $x_1$, $x_2$ при которых система оборачивается в нуль. Это называется решение СНУ.

\section{Обозначения}

Заменим в \eqref{dano} аргументы на вектор $\mathbf{x} = (x_1,\ x_2)$:

\begin{equation}\label{dano1}
\left\{
\begin{aligned}
	&F_1(\mathbf{x}) = 0 \\
	&F_2(\mathbf{x}) = 0
\end{aligned}
\right.
\end{equation}

Преобразуем \eqref{dano1} к одной вектор-функции $\mathbf{F}$, такой что $ \mathbf{F} : \mathbb{R}^2 \to \mathbb{R}^2 $:

$$\mathbf{F}(\mathbf{x}) = \roubr{
\begin{aligned}
	&F_1(\mathbf{x})\\
	&F_2(\mathbf{x})\\
\end{aligned}
}
$$

Тогда \eqref{dano} примет вид:

\begin{equation}\label{dano2}
\mathbf{F}(\mathbf{x}) = \mathbf{0}
\end{equation}

где $\mathbf{0}= (0,\ 0)^T$.

Под \textbf{невязкой} понимается вектор $\mathbf{F}(\mathbf{x})$.

\end{multicols}

\section{Преобразования}

\subsection{Разложение в ряд Тейлора для функции многих переменных}

Взято с \href{https://en.wikipedia.org/wiki/Taylor_series#Taylor_series_in_several_variables}{Википедии}.

Пусть дана функция $f(\mathbf{x}) : \mathbb{R}^2 \to \mathbb{R}$, $\mathbf{x} = (x_1,\ x_2)^T$.

Разложение функции $f(\mathbf{x})$ будет произведено в окрестности точки $\mathbf{a} = (a_1,\ a_2)^T$. Требуется, чтобы у функции $f$ имелись непрерывные производные до $(2+1)$-го порядка включительно по всем переменным.

Тогда разложение в ряд Тейлора имеет следующий вид:

\begin{equation}
f(\mathbf{x}) = f(\mathbf{a}) + \sum_{i = 1}^n \frac{\partial f(\mathbf{a})}{\partial x_i} (x_i - a_i) + \frac{1}{2!} \sum_{i = 1}^2\sum_{j = 1}^2 \frac{\partial f(\mathbf{a})}{\partial x_i \partial x_j} (x_i - a_i) (x_j - a_j) + R_3(\mathbf{x}) 
\end{equation}

Раскроем сумму первых двух членов предыдущего равенства:

\begin{equation}\label{taylor}
f(\mathbf{x}) = f(\mathbf{a}) + \frac{\partial f(\mathbf{a})}{\partial x_1} (x_1 - a_1) + \frac{\partial f(\mathbf{a})}{\partial x_2} (x_2 - a_2) + R_2(\mathbf{x})
\end{equation}

\begin{equation}\label{rtozero}
\text{Где } R_n(\mathbf{x}) \text{ --- остаточный член в форме Лагранжа, } R_n(\mathbf{x}) \xrightarrow{n\to \infty} 0 \text{ .}
\end{equation}

\subsection{Разложение в ряд Тейлора СНУ}

Обозначим за $\hat{\mathbf{x}}$ искомое решение, за $\mathbf{x}^{[k]}$ решение на $k$-м шаге и введем ещё обозначение смещения $\Delta\mathbf{x}^{[k]} = \hat{\mathbf{x}} - \mathbf{x}^{[k]}$.

Разложим $F_i\roubr{\mathbf{x}}$ в окрестности точки $\mathbf{x}^{[k]}$ в ряд Тейлора по формуле \eqref{taylor}:

$$ F_i\roubr{\mathbf{x}} = F_i\roubr{\mathbf{x}^{[k]}} + \frac{\partial F_i\roubr{\mathbf{x}^{[k]}}}{\partial x_1} \roubr{x_1 - x^{[k]}_1} + \frac{\partial F_i\roubr{\mathbf{x}^{[k]}}}{\partial x_2} \roubr{x_2 - x^{[k]}_2} + R_2\roubr{\mathbf{x}} $$

Подставив в это разложение точку $\hat{\mathbf{x}}$ получаем следующее:

$$\displaystyle F_i\roubr{\hat{\mathbf{x}}} = F_i\roubr{\mathbf{x}^{[k]}} + \frac{\partial F_i\roubr{\mathbf{x}^{[k]}}}{\partial x_1} \Delta x^{[k]}_1 + \frac{\partial F_i\roubr{\mathbf{x}^{[k]}}}{\partial x_2} \Delta x^{[k]}_2 + R_2\roubr{\hat{\mathbf{x}}} $$

Теперь запишем \eqref{dano2} с учетом предыдущего разложения:

$$ \left\{
\begin{aligned}
& F_1\roubr{\mathbf{x}^{[k]}} + \frac{\partial F_1\roubr{\mathbf{x}^{[k]}}}{\partial x_1} \Delta x^{[k]}_1 + \frac{\partial F_1\roubr{\mathbf{x}^{[k]}}}{\partial x_2} \Delta x^{[k]}_2 + R^{F_1}_2\roubr{\hat{\mathbf{x}}} = 0 \\
& F_2\roubr{\mathbf{x}^{[k]}} + \frac{\partial F_2\roubr{\mathbf{x}^{[k]}}}{\partial x_1} \Delta x^{[k]}_1 + \frac{\partial F_2\roubr{\mathbf{x}^{[k]}}}{\partial x_2} \Delta x^{[k]}_2 + R^{F_2}_2\roubr{\hat{\mathbf{x}}} = 0
\end{aligned}
\right.$$

Или в матричном виде:

$$
\mathbf{F}\roubr{\mathbf{x}^{[k]}} + 
\begin{pmatrix}
\displaystyle \frac{\partial F_1}{\partial x_1} & \displaystyle \frac{\partial F_1}{\partial x_2} \\ \\
\displaystyle \frac{\partial F_2}{\partial x_1} & \displaystyle \frac{\partial F_2}{\partial x_2}
\end{pmatrix} \roubr{\mathbf{x}^{[k]}} \cdot \Delta \mathbf{x}^{[k]} + \begin{pmatrix}
\displaystyle R^{F_1}_2\roubr{\hat{\mathbf{x}}} \\
\displaystyle R^{F_2}_2\roubr{\hat{\mathbf{x}}}
\end{pmatrix} = \mathbf{0}
$$

$$
\mathbf{F}\roubr{\mathbf{x}^{[k]}} + \mathbf{J}_\mathbf{F}\roubr{\mathbf{x}^{[k]}} \cdot \Delta \mathbf{x}^{[k]} + \mathbf{R}_2\roubr{\hat{\mathbf{x}}} = \mathbf{0}
$$

$$
\mathbf{J}_\mathbf{F}\roubr{\mathbf{x}^{[k]}} \cdot \Delta \mathbf{x}^{[k]} = -\mathbf{F}\roubr{\mathbf{x}^{[k]}} - \mathbf{R}_2\roubr{\hat{\mathbf{x}}}
$$

Где $\mathbf{J}_\mathbf{F}\roubr{\mathbf{x}}$ --- матрица Якоби функции $\mathbf{F}$ в точке $\mathbf{x}$.

Из этого уравнения можно найти $\Delta\mathbf{x}^{[k]}$, который прибавляется к текущему решению $\mathbf{x}^{[k]}$ чтобы найти $\hat{\mathbf{x}}$. 

Мы не не знаем как вычислять $\mathbf{R}_2\roubr{\hat{\mathbf{x}}}$, но можем им пренебречь, так как он стремится к нулю. Именно из-за этого принебрежения мы не сразу получаем точное решение, а процесс становится итеративным. Итог:

\begin{equation}\label{slau}
\mathbf{J}_\mathbf{F}\roubr{\mathbf{x}^{[k]}} \cdot \Delta \mathbf{x}^{[k]} = -\mathbf{F}\roubr{\mathbf{x}^{[k]}}
\end{equation}

Можно заметить, что это СЛАУ. Её можно легко решить известными методами.

\section{Алгоритм}

Весь метод заключается в итерационном процессе:

$$
\mathbf{x}^{[k+1]} = \mathbf{x}^{[k]} + \Delta \mathbf{x}^{[k]}
$$

\begin{equation}\label{iter}
\mathbf{x}^{[k+1]} = \mathbf{x}^{[k]} - \roubr{\mathbf{J}_\mathbf{F}\roubr{\mathbf{x}^{[k]}}}^{-1} \mathbf{F}\roubr{\mathbf{x}^{[k]}}
\end{equation}

Далее за $k$ обозначается количество итераций; за $m$ максимальное число итераций; за $\varepsilon_1$ максимальная допустимая невязка;

\begin{algorithm}[H]
\DontPrintSemicolon
\KwIn{$\varepsilon_1$, $m$, $\mathbf{F}$, $\mathbf{x}^{[0]}$}
\KwResult{$\mathbf{x}$}
\Begin{
	$\mathbf{x} \leftarrow \mathbf{x}^{[0]}$\;
	$k \leftarrow 0$\;
	\While{$||\mathbf{F}\roubr{\mathbf{x}}|| > \varepsilon_1$ \KwAnd $k < m$}{
		$\mathbf{A} \leftarrow \mathbf{J}_\mathbf{F}\roubr{\mathbf{x}}$\;
		$\mathbf{b} \leftarrow -\mathbf{F}\roubr{\mathbf{x}}$\;
		Решить СЛАУ $\mathbf{A}\cdot \Delta\mathbf{x} = \mathbf{b}$, результат поместить в $\Delta\mathbf{x}$\;
		$\mathbf{x} \leftarrow \mathbf{x} + \Delta\mathbf{x}$\;
		$k \leftarrow k + 1$\;
	}
}
\caption{Метод Ньютона}
\end{algorithm}

Выход из итерационного процесса производится когда невязка достигает необходимой нижней границы, либо превышается максимальное число итераций.

В $||\mathbf{F}\roubr{\mathbf{x}}|| > \varepsilon_1$ невязка считается через норму, потому что так проще сравнить вектор с одним числом. Норма выбирается по вашему усмотрению.

\textit{Замечание:} данный алгоритм не зависит от размерности системы, а, следовательно, может быть применен к любой размерности исходной СНУ.

\textit{Замечание:} в алгоритме используется матрица Якоби в известной точке, что означает, что знать её аналитическое выражение не нужно, её можно вычислять при помощи численного взятия производной в конкретной точке.

\subsection{Улучшенный метод Ньютона}

Иногда может получаться такой $\Delta \mathbf{x}$, что невязка в векторе $\mathbf{x}^{[k+1]}$ будет больше, чем в $\mathbf{x}^{[k]}$, поэтому вводится коэффициент $\beta \in \sqbr{0, 1}$, который домножается на $\Delta \mathbf{x}$, причем $\beta$ подбирается такой, чтобы невязка строго уменьшалась. Теперь \eqref{iter} выглядит следующим образом:

$$
\mathbf{x}^{[k+1]} = \mathbf{x}^{[k]} + \beta \cdot \Delta \mathbf{x}^{[k]}
$$

Чтобы найти такой $\beta$, при котором невязка будет уменьшаться, используется аналог двоичного поиска: изначально $\beta = 1$, затем, пока невязка нового решения больше, чем предыдущего, $\beta$ уменьшается вдвое. 

Так же, если $\beta \to 0$, то это означает, что невязка не может быть уменьшена на текущей итерации, и это ещё один повод завершить итерационный процесс.

С добавлением $\beta$

\begin{algorithm}[H]
\DontPrintSemicolon
\KwIn{$\varepsilon_1$, $\varepsilon_2$, $m$, $\mathbf{F}$, $\mathbf{x}^{[0]}$}
\KwResult{$\mathbf{x}$}
\Begin{
	$\mathbf{x} \leftarrow \mathbf{x}^{[0]}$\;
	$\beta \leftarrow 1$\;
	$k \leftarrow 0$\;
	\While{$||\mathbf{F}\roubr{\mathbf{x}}|| > \varepsilon_1$ \KwAnd $\beta > \varepsilon_2$ \KwAnd $k < m$}{
		$\mathbf{A} \leftarrow \mathbf{J}_\mathbf{F}\roubr{\mathbf{x}}$\;
		$\mathbf{b} \leftarrow -\mathbf{F}\roubr{\mathbf{x}}$\;
		Решить СЛАУ $\mathbf{A}\cdot \Delta \mathbf{x} = \mathbf{b}$, результат поместить в $\Delta \mathbf{x}$\;
		$\beta \leftarrow 1$\;
		\While{$||\mathbf{F}\roubr{\mathbf{x} + \beta \Delta\mathbf{x}}|| > ||\mathbf{F}\roubr{\mathbf{x}}||$ \KwAnd $\beta > \varepsilon_2$}{
			$\beta \leftarrow 0.5 \beta$\;
		}
		$\mathbf{x} \leftarrow \mathbf{x}^{[k]} + \beta\cdot \Delta \mathbf{x}$\;
		$k \leftarrow k + 1$\;
	}
}
\caption{Метод Ньютона}
\end{algorithm}

\subsection{Одномерный случай}

В одномерном случае \eqref{iter} преобразуется к:

$$ x^{[k+1]} = x^{[k]} - \roubr{\frac{d f\roubr{x^{[k]}}}{dx}}^{-1} f\roubr{x^{[k]}} = x^{[k]} - \frac{f\roubr{x^{[k]}}}{f'\roubr{x^{[k]}}} $$

\section{Пример}

Например, у нас имеется следующая СНУ:

$$\left\{
\begin{aligned}
	&(x-x_1)^2 + (y-y_1)^2 = (r_1 + r)^2 \\
	&(x-x_2)^2 + (y-y_2)^2 = (r_1 + r)^2 \\
	&\cfrac{|ax + by + c|}{\sqrt{a^2 + b^2}} = r
\end{aligned}
\right.
$$

Решением этой СНУ является окружность с центром в точке $(x,\ y)$ и радиусом $r$, которая касается внешним образом окружностей $(x_1,\ y_1,\ r_1)$, $(x_2,\ y_2,\ r_2)$ и прямой $ax + by + c = 0$.

Для этой системы получим:

$$
\mathbf{F} = \begin{pmatrix}
	(x-x_1)^2 + (y-y_1)^2 - (r_1 + r)^2 \\
	(x-x_2)^2 + (y-y_2)^2 - (r_1 + r)^2 \\
	\cfrac{|ax + by + c|}{\sqrt{a^2 + b^2}} - r
\end{pmatrix},\
\mathbf{J}_\mathbf{F} = \begin{pmatrix}
	2(x-x_1) & 2(y-y_1) & -2(r_1 + r) \\
	2(x-x_2) & 2(y-y_2) & -2(r_2 + r) \\
	\cfrac{|a|}{\sqrt{a^2 + b^2}} & \cfrac{|b|}{\sqrt{a^2 + b^2}} & -1
\end{pmatrix}
$$

Далее остается только применить алгоритм.

\section{Когда размерность СНУ не соответствует размерности решения}

Пусть дана СНУ:

$$\left\{
\begin{aligned}
	&F_1(x_1,\ x_2,\ \dots,\ x_n) = 0 \\
	&F_2(x_1,\ x_2,\ \dots,\ x_n) = 0 \\
	&\dots \\
	&F_m(x_1,\ x_2,\ \dots,\ x_n) = 0
\end{aligned}
\right.
$$

Вышеописанный алгоритм работает только когда $m=n$, в случае же $m\neq n$ есть несколько методов приведения к системы квадратному виду.

Следующие методы показывают как надо модифицировать СНУ для конкретной точки, поэтому не получится их применить для того, чтобы один раз вычислить новую СНУ, её придётся считать на каждой итерации заного.

Новая СНУ и соответствующая ей матрица Якоби формируется только для поиска $\Delta\mathbf{x}$ при решении СЛАУ, для расчета невязки используется изначальная СНУ $\mathbf{F}$.

\subsection{Модифицированный алгоритм}

\begin{algorithm}[H]
\DontPrintSemicolon
\KwIn{$\varepsilon_1$, $\varepsilon_2$, $m$, $\mathbf{F}$, $\mathbf{x}^{[0]}$, $\mathbb{S}$, $\mathbb{J}$}
\KwResult{$\mathbf{x}^{[k]}$}
\Begin{
	$\mathbf{x}^{[k]} \leftarrow \mathbf{x}^{[0]}$\;
	$\beta \leftarrow 1$\;
	$k \leftarrow 0$\;
	\While{$||\mathbf{F}\roubr{\mathbf{x}}|| > \varepsilon_1$ и $\beta > \varepsilon_2$ и $k < m$}{
		$\mathbf{G} \leftarrow \mathbb{S}\roubr{\mathbf{F},\ \mathbf{x}^{[k]}}$\;
		$\mathbf{I} \leftarrow \mathbb{J}\roubr{\mathbf{F},\ \mathbf{G},\ \mathbf{x}^{[k]}}$\;
		$\mathbf{A} \leftarrow \mathbf{I}\roubr{\mathbf{x}^{[k]}}$\;
		$\mathbf{b} \leftarrow -\mathbf{G}\roubr{\mathbf{x}^{[k]}}$\;
		Решить СЛАУ $\mathbf{A}\cdot \mathbf{x} = \mathbf{b}$, результат поместить в $\mathbf{x}$\;
		$\beta \leftarrow 1$\;
		\While{$||\mathbf{F}\roubr{\mathbf{x}^{[k]} + \beta \mathbf{x}}|| > ||\mathbf{F}\roubr{\mathbf{x}^{[k]}}||$ и $\beta > \varepsilon_2$}{
			$\beta \leftarrow 0.5 \beta$\;
		}
		$\mathbf{x}^{[k]} \leftarrow \mathbf{x}^{[k]} + \beta \mathbf{x}$\;
		$k \leftarrow k + 1$\;
	}
}
\caption{Метод Ньютона для $m \neq n$}
\end{algorithm}

$\mathbb{S}$ --- это функция, преобразующая $\mathbf{F}$ к размерности $n$ в конкретной точке. О видах этой функции будет написано далее. В случае, когда размерность СНУ совпадает с размерностью решения, $\mathbb{S}(\mathbf{F},\ \mathbf{x}) = \mathbf{F}$.

Аналогично $\mathbb{J}$ --- функция возвращающая матрицу Якоби для решения СЛАУ в конкретной точке. Если не указано, как её вычислять, то $\mathbb{J}(\mathbf{F},\ \mathbf{G},\ \mathbf{x}) = \mathbf{J}_{\mathbf{G}}$.

Причем модификации затрагивают только часть, относящуюся к решению СЛАУ, но не затрагивают часть для вычисления нормы (невязки). Так сделано, потому что всё же решается функция $\mathbf{F}$, а не $\mathbf{G}$. 

\subsection{$m < n$}

Это означает, что решений, где СНУ оборачивается в ноль, множество.

Новая СНУ $\mathbf{G}(\mathbf{x})$ строится по алгоритму:

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{$\mathbf{F}$, $\mathbf{a}$}
\KwResult{$\mathbf{G}$}
\Begin{
	$\mathbf{A} \leftarrow \mathbf{J}_\mathbf{F}\roubr{\mathbf{a}}$\;
	$\displaystyle \mathbf{y} \leftarrow \roubr{\max_i |A_{i, 1}|,\ \max_i |A_{i, 2}|,\ \dots,\ \max_i |A_{i, n}|}$\;
	Находятся такие $i_j$, что: $y_{i_1} \geqslant y_{i_2} \geqslant \dots \geqslant y_{i_n}$\;
	\For{$j \leftarrow 1$ \KwTo $ n - m $}{
		$G_j(\mathbf{x}) \leftarrow x_{i_j} - a_{i_j}$\;
	}
	\For{$j \leftarrow n - m + 1$ \KwTo $ n $}{
		$G_j(\mathbf{x}) \leftarrow F_{i_j}(\mathbf{x}) $\;
	}
}
\caption{$m < n,\ \mathbb{S}_1\roubr{\mathbf{F},\ \mathbf{a}}$\label{S_1}}
\end{algorithm}

Обратите внимание, что в итоге $\mathbf{G}(\mathbf{x})$ является функцией $\mathbf{G} : \mathbb{R}^n \to \mathbb{R}^n$ и для каждой различной точки $\mathbf{a}$ функция $\mathbf{G}$ различна.

Такое преобразование делается для того, чтобы при нахождении $\Delta \mathbf{x}$ компоненты с минимальными частными производными были равны нулю.

\subsection{$m > n$}

Это означает, что решений может не быть, в данном случае метод Ньютона находит точку с минимальной невязкой.

\subsubsection{Метод исключения}

Из системы исключаются те уравнения, для которых $F_i(\mathbf{x})$ минимальны.

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{$\mathbf{F}$, $\mathbf{a}$}
\KwResult{$\mathbf{G}$}
\Begin{
	$\displaystyle \mathbf{y} \leftarrow \roubr{|F_1(\mathbf{x})|,\ |F_2(\mathbf{x})|,\ \dots,\ |F_m(\mathbf{x})|}$\;
	Находятся такие $i_j$, что: $y_{i_1} \geqslant y_{i_2} \geqslant \dots \geqslant y_{i_m}$\;
	\For{$j \leftarrow 1$ \KwTo $ n - m $}{
		$G_j(\mathbf{x}) \leftarrow F_{i_j}(\mathbf{x}) $\;
	}
}
\caption{$m > n,\ \mathbb{S}_2\roubr{\mathbf{F},\ \mathbf{a}}$\label{S_2}}
\end{algorithm}

\subsubsection{Метод свертки}

Для всех лишних уравнений у которых $F_i(\mathbf{x})$ минимальны, проводится свертка. Вместо исключаемых уравнений берется одно уравнение с суммой квадратов исключаемых уравнений.

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{$\mathbf{F}$, $\mathbf{a}$}
\KwResult{$\mathbf{G}$}
\Begin{
	$\displaystyle \mathbf{y} \leftarrow \roubr{|F_1(\mathbf{x})|,\ |F_2(\mathbf{x})|,\ \dots,\ |F_m(\mathbf{x})|}$\;
	Находятся такие $i_j$, что: $y_{i_1} \geqslant y_{i_2} \geqslant \dots \geqslant y_{i_m}$\;
	\For{$j \leftarrow 1$ \KwTo $ n - m - 1 $}{
		$G_j(\mathbf{x}) \leftarrow F_{i_j}(\mathbf{x}) $\;
	}
	$\displaystyle G_n \leftarrow \sum_{j = n - m}^{n} F_{i_j}^2(\mathbf{x})$\;
}
\caption{$m > n,\ \mathbb{S}_3\roubr{\mathbf{F},\ \mathbf{a}}$\label{S_3}}
\end{algorithm}

\subsubsection{Процедура симметризации}

Вместо исходной системы должна решаться система:

$$ \mathbb{S}_4\roubr{\mathbf{F},\ \mathbf{x}^{[k]}} = \roubr{\mathbf{J}_\mathbf{F}\roubr{\mathbf{x}^{[k]}}}^T \cdot \mathbf{F}\roubr{\mathbf{x}^{[k]}} $$

$$ \mathbb{J}_4\roubr{\mathbf{F},\ \mathbf{G},\ \mathbf{x}^{[k]}} = \roubr{\mathbf{J}_\mathbf{F}\roubr{\mathbf{x}^{[k]}}}^T \cdot \mathbf{J}_\mathbf{F}\roubr{\mathbf{x}^{[k]}} $$

\textit{Замечание:} только ради этой процедуры в аргументах $\mathbb{J}$ имеется $\mathbf{F}$, и в принципе только ради этой процедуры введена функция $\mathbb{J}$. Причем $\mathbf{G}$ здесь не используется.

\end{document}